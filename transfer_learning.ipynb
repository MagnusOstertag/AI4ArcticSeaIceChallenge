{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the model with learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from unet import UNet  # Convolutional Neural Network model\n",
    "from functions import chart_cbar, r2_metric, f1_metric, compute_metrics  # Functions to calculate metrics and show the relevant chart colorbar.\n",
    "from utils import CHARTS, SIC_LOOKUP, SOD_LOOKUP, FLOE_LOOKUP, SCENE_VARIABLES, colour_str\n",
    "\n",
    "%store -r train_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_options = {\n",
    "    # -- Training options -- #\n",
    "    'path_to_processed_data': os.environ['AI4ARCTIC_DATA'],  # Replace with data directory path.\n",
    "    'path_to_env': os.environ['AI4ARCTIC_ENV'],  # Replace with environmment directory path.\n",
    "    'lr': 0.0001,  # Optimizer learning rate.\n",
    "    'epochs': 50,  # Number of epochs before training stop.\n",
    "    'epoch_len': 500,  # Number of batches for each epoch.\n",
    "    'patch_size': 256,  # Size of patches sampled. Used for both Width and Height.\n",
    "    'batch_size': 8,  # Number of patches for each batch.\n",
    "    'loader_upsampling': 'nearest',  # How to upscale low resolution variables to high resolution.\n",
    "    \n",
    "    # -- Data prepraration lookups and metrics.\n",
    "    'train_variables': SCENE_VARIABLES,  # Contains the relevant variables in the scenes.\n",
    "    'charts': CHARTS,  # Charts to train on.\n",
    "    'n_classes': {  # number of total classes in the reference charts, including the mask.\n",
    "        'SIC': SIC_LOOKUP['n_classes'],\n",
    "        'SOD': SOD_LOOKUP['n_classes'],\n",
    "        'FLOE': FLOE_LOOKUP['n_classes']\n",
    "    },\n",
    "    'pixel_spacing': 80,  # SAR pixel spacing. 80 for the ready-to-train AI4Arctic Challenge dataset.\n",
    "    'train_fill_value': 0,  # Mask value for SAR training data.\n",
    "    'class_fill_values': {  # Mask value for class/reference data.\n",
    "        'SIC': SIC_LOOKUP['mask'],\n",
    "        'SOD': SOD_LOOKUP['mask'],\n",
    "        'FLOE': FLOE_LOOKUP['mask'],\n",
    "    },\n",
    "    \n",
    "    # -- Validation options -- #\n",
    "    'chart_metric': {  # Metric functions for each ice parameter and the associated weight.\n",
    "        'SIC': {\n",
    "            'func': r2_metric,\n",
    "            'weight': 2,\n",
    "        },\n",
    "        'SOD': {\n",
    "            'func': f1_metric,\n",
    "            'weight': 2,\n",
    "        },\n",
    "        'FLOE': {\n",
    "            'func': f1_metric,\n",
    "            'weight': 1,\n",
    "        },\n",
    "    },\n",
    "    'num_val_scenes': 10,  # Number of scenes randomly sampled from train_list to use in validation.\n",
    "    \n",
    "    # -- GPU/cuda options -- #\n",
    "    'gpu_id': 0,  # Index of GPU. In case of multiple GPUs.\n",
    "    'num_workers': 6,  # Number of parallel processes to fetch data.\n",
    "    'num_workers_val': 1,  # Number of parallel processes during validation.\n",
    "    \n",
    "    # -- U-Net Options -- #\n",
    "    'unet_conv_filters': [16, 32, 32, 32],  # Number of filters in the U-Net.\n",
    "    'conv_kernel_size': (3, 3),  # Size of convolutional kernels.\n",
    "    'conv_stride_rate': (1, 1),  # Stride rate of convolutional kernels.\n",
    "    'conv_dilation_rate': (1, 1),  # Dilation rate of convolutional kernels.\n",
    "    'conv_padding': (1, 1),  # Number of padded pixels in convolutional layers.\n",
    "    'conv_padding_style': 'zeros',  # Style of padding.\n",
    "\n",
    "    # -- Transfer learning options -- #\n",
    "    'transfer_learning': False,  # Whether to use transfer learning.\n",
    "    'transfer_model_architecture': {'unet_conv_filters': [16, 32, 32, 32],}, # Dict of the differences in the U-Net options of the model architecture.\n",
    "    'transfer_model_path': 'archive/model-first_run_4lvl',  # Path to the model to transfer from.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mGPU not available.\u001b[0m\n",
      "GPU setup complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Get GPU resources.\n",
    "if torch.cuda.is_available():\n",
    "    print(colour_str('GPU available!', 'green'))\n",
    "    print('Total number of available devices: ', colour_str(torch.cuda.device_count(), 'orange'))\n",
    "    device = torch.device(f\"cuda:{train_options['gpu_id']}\")\n",
    "\n",
    "else:\n",
    "    print(colour_str('GPU not available.', 'red'))\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('GPU setup complete.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to load the model saved in `quickstart.ipynb` with the function\n",
    "\n",
    "\n",
    "`torch.save(obj={'model_state_dict': net.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'epoch': epoch},\n",
    "                        f=f\"{train_options['model_codename']}_{epoch}_best_model\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/work/Dokumente/Studium/SimTech_MSc/Erasmus/Lectures/AI_in_industry/project/AI4ArcticSeaIceChallenge/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_options['path_to_env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['input_block.double_conv.0.weight', 'input_block.double_conv.1.weight', 'input_block.double_conv.1.bias', 'input_block.double_conv.1.running_mean', 'input_block.double_conv.1.running_var', 'input_block.double_conv.1.num_batches_tracked', 'input_block.double_conv.3.weight', 'input_block.double_conv.4.weight', 'input_block.double_conv.4.bias', 'input_block.double_conv.4.running_mean', 'input_block.double_conv.4.running_var', 'input_block.double_conv.4.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.0.weight', 'contract_blocks.0.double_conv.double_conv.1.weight', 'contract_blocks.0.double_conv.double_conv.1.bias', 'contract_blocks.0.double_conv.double_conv.1.running_mean', 'contract_blocks.0.double_conv.double_conv.1.running_var', 'contract_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.0.double_conv.double_conv.3.weight', 'contract_blocks.0.double_conv.double_conv.4.weight', 'contract_blocks.0.double_conv.double_conv.4.bias', 'contract_blocks.0.double_conv.double_conv.4.running_mean', 'contract_blocks.0.double_conv.double_conv.4.running_var', 'contract_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.0.weight', 'contract_blocks.1.double_conv.double_conv.1.weight', 'contract_blocks.1.double_conv.double_conv.1.bias', 'contract_blocks.1.double_conv.double_conv.1.running_mean', 'contract_blocks.1.double_conv.double_conv.1.running_var', 'contract_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.1.double_conv.double_conv.3.weight', 'contract_blocks.1.double_conv.double_conv.4.weight', 'contract_blocks.1.double_conv.double_conv.4.bias', 'contract_blocks.1.double_conv.double_conv.4.running_mean', 'contract_blocks.1.double_conv.double_conv.4.running_var', 'contract_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.0.weight', 'contract_blocks.2.double_conv.double_conv.1.weight', 'contract_blocks.2.double_conv.double_conv.1.bias', 'contract_blocks.2.double_conv.double_conv.1.running_mean', 'contract_blocks.2.double_conv.double_conv.1.running_var', 'contract_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'contract_blocks.2.double_conv.double_conv.3.weight', 'contract_blocks.2.double_conv.double_conv.4.weight', 'contract_blocks.2.double_conv.double_conv.4.bias', 'contract_blocks.2.double_conv.double_conv.4.running_mean', 'contract_blocks.2.double_conv.double_conv.4.running_var', 'contract_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'bridge.double_conv.double_conv.0.weight', 'bridge.double_conv.double_conv.1.weight', 'bridge.double_conv.double_conv.1.bias', 'bridge.double_conv.double_conv.1.running_mean', 'bridge.double_conv.double_conv.1.running_var', 'bridge.double_conv.double_conv.1.num_batches_tracked', 'bridge.double_conv.double_conv.3.weight', 'bridge.double_conv.double_conv.4.weight', 'bridge.double_conv.double_conv.4.bias', 'bridge.double_conv.double_conv.4.running_mean', 'bridge.double_conv.double_conv.4.running_var', 'bridge.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.0.weight', 'expand_blocks.0.double_conv.double_conv.1.weight', 'expand_blocks.0.double_conv.double_conv.1.bias', 'expand_blocks.0.double_conv.double_conv.1.running_mean', 'expand_blocks.0.double_conv.double_conv.1.running_var', 'expand_blocks.0.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.0.double_conv.double_conv.3.weight', 'expand_blocks.0.double_conv.double_conv.4.weight', 'expand_blocks.0.double_conv.double_conv.4.bias', 'expand_blocks.0.double_conv.double_conv.4.running_mean', 'expand_blocks.0.double_conv.double_conv.4.running_var', 'expand_blocks.0.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.0.weight', 'expand_blocks.1.double_conv.double_conv.1.weight', 'expand_blocks.1.double_conv.double_conv.1.bias', 'expand_blocks.1.double_conv.double_conv.1.running_mean', 'expand_blocks.1.double_conv.double_conv.1.running_var', 'expand_blocks.1.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.1.double_conv.double_conv.3.weight', 'expand_blocks.1.double_conv.double_conv.4.weight', 'expand_blocks.1.double_conv.double_conv.4.bias', 'expand_blocks.1.double_conv.double_conv.4.running_mean', 'expand_blocks.1.double_conv.double_conv.4.running_var', 'expand_blocks.1.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.0.weight', 'expand_blocks.2.double_conv.double_conv.1.weight', 'expand_blocks.2.double_conv.double_conv.1.bias', 'expand_blocks.2.double_conv.double_conv.1.running_mean', 'expand_blocks.2.double_conv.double_conv.1.running_var', 'expand_blocks.2.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.2.double_conv.double_conv.3.weight', 'expand_blocks.2.double_conv.double_conv.4.weight', 'expand_blocks.2.double_conv.double_conv.4.bias', 'expand_blocks.2.double_conv.double_conv.4.running_mean', 'expand_blocks.2.double_conv.double_conv.4.running_var', 'expand_blocks.2.double_conv.double_conv.4.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.0.weight', 'expand_blocks.3.double_conv.double_conv.1.weight', 'expand_blocks.3.double_conv.double_conv.1.bias', 'expand_blocks.3.double_conv.double_conv.1.running_mean', 'expand_blocks.3.double_conv.double_conv.1.running_var', 'expand_blocks.3.double_conv.double_conv.1.num_batches_tracked', 'expand_blocks.3.double_conv.double_conv.3.weight', 'expand_blocks.3.double_conv.double_conv.4.weight', 'expand_blocks.3.double_conv.double_conv.4.bias', 'expand_blocks.3.double_conv.double_conv.4.running_mean', 'expand_blocks.3.double_conv.double_conv.4.running_var', 'expand_blocks.3.double_conv.double_conv.4.num_batches_tracked', 'sic_feature_map.feature_out.weight', 'sic_feature_map.feature_out.bias', 'sod_feature_map.feature_out.weight', 'sod_feature_map.feature_out.bias', 'floe_feature_map.feature_out.weight', 'floe_feature_map.feature_out.bias'])\n",
      "42\n",
      "dict_keys(['state', 'param_groups'])\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('archive/model-first_run_4lvl')\n",
    "print(model['model_state_dict'].keys())\n",
    "print(model['epoch'])\n",
    "print(model['optimizer_state_dict'].keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our 4-conv-filters Net `[16, 32, 32, 32]` which we can see when counting the four contract_blocks and the four expand_blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "unet: input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "weights: input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "unet: input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "weights: contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "unet: contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "weights: contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: expand_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "unet: expand_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "weights: expand_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: expand_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "unet: expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "weights: expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "unet: expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "weights: expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "unet: expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "weights: expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "unet: expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "weights: expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "unet: expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "weights: sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "unet: sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "weights: sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "unet: sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "weights: floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n",
      "unet: floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# I want to set the weights of the first 4 layers to the weights of the first 4 layers of the smaller net.\n",
    "# I want to set the weights of the last 4 layers to the weights of the last 4 layers of the smaller net.\n",
    "\n",
    "old_weights = list(model['model_state_dict'])\n",
    "\n",
    "net = UNet(options=train_options)\n",
    "weights_new = net.state_dict()\n",
    "\n",
    "for name, tensor in model['model_state_dict'].items():\n",
    "    if len(tensor.shape) > 1:\n",
    "        print(f\"weights: {name} and {tensor.shape}\")\n",
    "        print(f\"unet: {name} and {weights_new[name].shape}\")\n",
    "        if tensor.shape != weights_new[name].shape:\n",
    "            print('Shape mismatch!')\n",
    "            print(f\"SHAPES: {tensor.shape} and {weights_new[name].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]           3,456\n",
      "       BatchNorm2d-2         [-1, 16, 256, 256]              32\n",
      "              ReLU-3         [-1, 16, 256, 256]               0\n",
      "            Conv2d-4         [-1, 16, 256, 256]           2,304\n",
      "       BatchNorm2d-5         [-1, 16, 256, 256]              32\n",
      "              ReLU-6         [-1, 16, 256, 256]               0\n",
      "        DoubleConv-7         [-1, 16, 256, 256]               0\n",
      "         MaxPool2d-8         [-1, 16, 128, 128]               0\n",
      "            Conv2d-9         [-1, 32, 128, 128]           4,608\n",
      "      BatchNorm2d-10         [-1, 32, 128, 128]              64\n",
      "             ReLU-11         [-1, 32, 128, 128]               0\n",
      "           Conv2d-12         [-1, 32, 128, 128]           9,216\n",
      "      BatchNorm2d-13         [-1, 32, 128, 128]              64\n",
      "             ReLU-14         [-1, 32, 128, 128]               0\n",
      "       DoubleConv-15         [-1, 32, 128, 128]               0\n",
      " ContractingBlock-16         [-1, 32, 128, 128]               0\n",
      "        MaxPool2d-17           [-1, 32, 64, 64]               0\n",
      "           Conv2d-18           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-19           [-1, 32, 64, 64]              64\n",
      "             ReLU-20           [-1, 32, 64, 64]               0\n",
      "           Conv2d-21           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-22           [-1, 32, 64, 64]              64\n",
      "             ReLU-23           [-1, 32, 64, 64]               0\n",
      "       DoubleConv-24           [-1, 32, 64, 64]               0\n",
      " ContractingBlock-25           [-1, 32, 64, 64]               0\n",
      "        MaxPool2d-26           [-1, 32, 32, 32]               0\n",
      "           Conv2d-27           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-28           [-1, 32, 32, 32]              64\n",
      "             ReLU-29           [-1, 32, 32, 32]               0\n",
      "           Conv2d-30           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 32, 32]              64\n",
      "             ReLU-32           [-1, 32, 32, 32]               0\n",
      "       DoubleConv-33           [-1, 32, 32, 32]               0\n",
      " ContractingBlock-34           [-1, 32, 32, 32]               0\n",
      "        MaxPool2d-35           [-1, 32, 16, 16]               0\n",
      "           Conv2d-36           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-37           [-1, 32, 16, 16]              64\n",
      "             ReLU-38           [-1, 32, 16, 16]               0\n",
      "           Conv2d-39           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-40           [-1, 32, 16, 16]              64\n",
      "             ReLU-41           [-1, 32, 16, 16]               0\n",
      "       DoubleConv-42           [-1, 32, 16, 16]               0\n",
      " ContractingBlock-43           [-1, 32, 16, 16]               0\n",
      "         Upsample-44           [-1, 32, 32, 32]               0\n",
      "           Conv2d-45           [-1, 32, 32, 32]          18,432\n",
      "      BatchNorm2d-46           [-1, 32, 32, 32]              64\n",
      "             ReLU-47           [-1, 32, 32, 32]               0\n",
      "           Conv2d-48           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-49           [-1, 32, 32, 32]              64\n",
      "             ReLU-50           [-1, 32, 32, 32]               0\n",
      "       DoubleConv-51           [-1, 32, 32, 32]               0\n",
      "   ExpandingBlock-52           [-1, 32, 32, 32]               0\n",
      "         Upsample-53           [-1, 32, 64, 64]               0\n",
      "           Conv2d-54           [-1, 32, 64, 64]          18,432\n",
      "      BatchNorm2d-55           [-1, 32, 64, 64]              64\n",
      "             ReLU-56           [-1, 32, 64, 64]               0\n",
      "           Conv2d-57           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-58           [-1, 32, 64, 64]              64\n",
      "             ReLU-59           [-1, 32, 64, 64]               0\n",
      "       DoubleConv-60           [-1, 32, 64, 64]               0\n",
      "   ExpandingBlock-61           [-1, 32, 64, 64]               0\n",
      "         Upsample-62         [-1, 32, 128, 128]               0\n",
      "           Conv2d-63         [-1, 32, 128, 128]          18,432\n",
      "      BatchNorm2d-64         [-1, 32, 128, 128]              64\n",
      "             ReLU-65         [-1, 32, 128, 128]               0\n",
      "           Conv2d-66         [-1, 32, 128, 128]           9,216\n",
      "      BatchNorm2d-67         [-1, 32, 128, 128]              64\n",
      "             ReLU-68         [-1, 32, 128, 128]               0\n",
      "       DoubleConv-69         [-1, 32, 128, 128]               0\n",
      "   ExpandingBlock-70         [-1, 32, 128, 128]               0\n",
      "         Upsample-71         [-1, 32, 256, 256]               0\n",
      "           Conv2d-72         [-1, 16, 256, 256]           6,912\n",
      "      BatchNorm2d-73         [-1, 16, 256, 256]              32\n",
      "             ReLU-74         [-1, 16, 256, 256]               0\n",
      "           Conv2d-75         [-1, 16, 256, 256]           2,304\n",
      "      BatchNorm2d-76         [-1, 16, 256, 256]              32\n",
      "             ReLU-77         [-1, 16, 256, 256]               0\n",
      "       DoubleConv-78         [-1, 16, 256, 256]               0\n",
      "   ExpandingBlock-79         [-1, 16, 256, 256]               0\n",
      "           Conv2d-80         [-1, 12, 256, 256]             204\n",
      "       FeatureMap-81         [-1, 12, 256, 256]               0\n",
      "           Conv2d-82          [-1, 7, 256, 256]             119\n",
      "       FeatureMap-83          [-1, 7, 256, 256]               0\n",
      "           Conv2d-84          [-1, 8, 256, 256]             136\n",
      "       FeatureMap-85          [-1, 8, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 168,523\n",
      "Trainable params: 168,523\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6.00\n",
      "Forward/backward pass size (MB): 256.06\n",
      "Params size (MB): 0.64\n",
      "Estimated Total Size (MB): 262.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(model['model_state_dict'])\n",
    "summary(net, input_size=(24, train_options['patch_size'], train_options['patch_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetTrans(\n",
       "  (input_block): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (contract_blocks): ModuleList(\n",
       "    (0): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ContractingBlock(\n",
       "      (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bridge): ContractingBlock(\n",
       "    (contract_block): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (double_conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand_blocks): ModuleList(\n",
       "    (0): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ExpandingBlock(\n",
       "      (upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "      (double_conv): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(48, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sic_feature_map): FeatureMap(\n",
       "    (feature_out): Conv2d(16, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sod_feature_map): FeatureMap(\n",
       "    (feature_out): Conv2d(16, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (floe_feature_map): FeatureMap(\n",
       "    (feature_out): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet_transfer import UNetTrans\n",
    "\n",
    "net_trans = UNetTrans(options=train_options)\n",
    "net_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n",
      "floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# check whether everything works, for loading a same size model\n",
    "weights_old = net.state_dict()\n",
    "\n",
    "weights_new = net_trans.state_dict()\n",
    "\n",
    "for name, tensor in weights_old.items():\n",
    "    if len(tensor.shape) > 1:\n",
    "        print(f\"{name} and {tensor.shape}\")\n",
    "        print(f\"{name} and {weights_new[name].shape}\")\n",
    "        if tensor.shape != weights_new[name].shape:\n",
    "            print('Shape mismatch!')\n",
    "            print(f\"SHAPES: {tensor.shape} and {weights_new[name].shape}\")\n",
    "        elif not torch.allclose(tensor, weights_new[name]):\n",
    "            print('Weights mismatch!')\n",
    "            print(f\"WEIGHTS old: {tensor[0,0,:,:]} \\nWEIGHTS new: {weights_new[name][0,0,:,:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD: input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "NEW: input_block.double_conv.0.weight and torch.Size([16, 24, 3, 3])\n",
      "OLD: input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "NEW: input_block.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "OLD: contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "NEW: contract_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 16, 3, 3])\n",
      "OLD: contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: contract_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: contract_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: contract_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: contract_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: contract_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: bridge.double_conv.double_conv.0.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: bridge.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: expand_blocks.0.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "NEW: expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "OLD: expand_blocks.0.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: expand_blocks.1.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "NEW: expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "OLD: expand_blocks.1.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: expand_blocks.2.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "NEW: expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([32, 64, 3, 3])\n",
      "OLD: expand_blocks.2.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "NEW: expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([32, 32, 3, 3])\n",
      "OLD: expand_blocks.3.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "NEW: expand_blocks.4.double_conv.double_conv.0.weight and torch.Size([16, 48, 3, 3])\n",
      "OLD: expand_blocks.3.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "NEW: expand_blocks.4.double_conv.double_conv.3.weight and torch.Size([16, 16, 3, 3])\n",
      "OLD: sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "NEW: sic_feature_map.feature_out.weight and torch.Size([12, 16, 1, 1])\n",
      "OLD: sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "NEW: sod_feature_map.feature_out.weight and torch.Size([7, 16, 1, 1])\n",
      "OLD: floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n",
      "NEW: floe_feature_map.feature_out.weight and torch.Size([8, 16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# check whether everything works, for loading a smaller model\n",
    "train_options['unet_conv_filters'] = [16, 32, 32, 32, 32]\n",
    "train_options['transfer_model_architecture']['unet_conv_filters'] = [16, 32, 32, 32]\n",
    "net_trans = UNetTrans(options=train_options)\n",
    "\n",
    "weights_new = net_trans.state_dict()\n",
    "\n",
    "for name, tensor in weights_old.items():\n",
    "    if len(tensor.shape) > 1:\n",
    "        # shift in the names of the expanding layer of the larger net\n",
    "        new_name = name\n",
    "        if 'expand_blocks' in name:\n",
    "            no_layer = int(name.split('.')[1])\n",
    "            new_name = f\"expand_blocks.{no_layer+1}.{name.split('.')[2]}.{name.split('.')[3]}.{name.split('.')[4]}.{name.split('.')[5]}\"\n",
    "        print(f\"OLD: {name} and {tensor.shape}\")\n",
    "        print(f\"NEW: {new_name} and {weights_new[new_name].shape}\")\n",
    "        if tensor.shape != weights_new[new_name].shape:\n",
    "            print('Shape mismatch!')\n",
    "            print(f\"SHAPES: {tensor.shape} and {weights_new[new_name].shape}\")\n",
    "        elif not torch.allclose(tensor, weights_new[new_name]):\n",
    "            print('Weights mismatch!')\n",
    "            print(f\"WEIGHTS old: {tensor[0,0,:,:]} \\nWEIGHTS new: {weights_new[new_name][0,0,:,:]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
